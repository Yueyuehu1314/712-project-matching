#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµæ´»æ¸…æ´çŸ¥è¯†å›¾è°±ä¿¡æ¯æå–ä»£ç†
æ”¾å®½åŒ¹é…æ¡ä»¶ï¼Œå¢å¼ºPDä¸UOæŠ€èƒ½äº¤é›†æ£€æµ‹
"""

import os
import json
import re
import string
from collections import Counter, defaultdict
from typing import Dict, List, Set, Tuple, Optional

class FlexibleCleanKGExtractor:
    """çµæ´»æ¸…æ´çŸ¥è¯†å›¾è°±æå–å™¨"""
    
    def __init__(self):
        # æ‰©å±•åŒä¹‰è¯æ˜ å°„è¡¨
        self.synonyms = {
            # AI/ML ç›¸å…³
            "ai": "machine learning",
            "artificial intelligence": "machine learning",
            "ml": "machine learning",
            "deep learning": "machine learning",
            "neural network": "machine learning",
            "pattern recognition": "machine learning",
            "computer vision": "machine learning",
            "data mining": "machine learning",
            
            # ç½‘ç»œç›¸å…³
            "wifi": "networking",
            "wireless": "networking", 
            "network": "networking",
            "internet": "networking",
            "protocol": "networking",
            "wifi channel": "networking",
            "csi": "networking",  # Channel State Information
            
            # æ•°æ®ç›¸å…³
            "data exploration": "data science",
            "data analysis": "data science",
            "data visualization": "data science",
            "analytics": "data science",
            "big data": "data science",
            
            # å¼€å‘ç›¸å…³
            "web dev": "web development",
            "frontend": "web development",
            "backend": "web development",
            "mobile": "mobile development",
            "app": "mobile development",
            
            # æ¥å£ç›¸å…³
            "ux": "user experience",
            "ui": "user interface",
            "hci": "human computer interaction",
            "human computer interaction": "user experience",
            
            # å®‰å…¨ç›¸å…³
            "cyber security": "cybersecurity",
            "information security": "cybersecurity",
            "security": "cybersecurity",
            
            # æ•°æ®åº“ç›¸å…³
            "db": "database",
            "sql": "database",
            "data management": "database",
            
            # å…¶ä»–æŠ€æœ¯
            "iot": "internet of things",
            "api": "programming",
            "algorithm": "programming",
            "coding": "programming"
        }
        
        # å‡å°‘è¿‡åº¦æ³›åŒ–æœ¯è¯­åˆ—è¡¨ï¼Œä¿ç•™æ›´å¤šæœ‰ç”¨è¯æ±‡
        self.generic_terms = {
            "computer", "software", "technology", "system", "application",
            "course", "unit", "subject", "study", "student", "learning"
        }
        
        # åœç”¨è¯
        self.stop_words = {
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "of", "with", "by", "is", "are", "was", "were", "be", "been", "have",
            "has", "had", "will", "would", "could", "should", "may", "might"
        }
        
        # QUTç¨‹åºæ˜ å°„
        self.program_mapping = {
            'IFN': 'Master of Information Technology',
            'CAB': 'Bachelor of Information Technology', 
            'IAB': 'Bachelor of Information Systems',
            'INB': 'Bachelor of Business Information Systems',
            'INN': 'Bachelor of Innovation',
            'ENN': 'Bachelor of Engineering',
            'MGN': 'Master of Business Administration',
            # æ·»åŠ ç¼ºå¤±çš„æ˜ å°„
            'QFN': 'QUT Foundation Program',
            'IGB': 'International Business Program',
            'MGZ': 'Management Program'
        }
        
        # å•å…ƒä»£ç æ¨¡å¼
        self.unit_pattern = r'\b[A-Z]{3}\d{3}\b'
        
        # æ”¾å®½å‚æ•°è®¾ç½®
        self.M = 8  # PDæŠ€èƒ½topæ•°é‡ (å¢åŠ )
        self.K = 5  # æ¯ä¸ªUNITæŠ€èƒ½topæ•°é‡ (å¢åŠ )
    
    def normalize_skill(self, skill: str) -> Optional[str]:
        """çµæ´»æ ‡å‡†åŒ–æŠ€èƒ½åç§°"""
        if not skill:
            return None
        
        skill_lower = skill.lower().strip()
        
        # è¿‡æ»¤è¿‡çŸ­æœ¯è¯­
        if len(skill_lower) < 2:  # æ”¾å®½åˆ°2ä¸ªå­—ç¬¦
            return None
        
        # æ£€æŸ¥åŒä¹‰è¯æ˜ å°„ (æ”¯æŒéƒ¨åˆ†åŒ¹é…)
        for original, canonical in self.synonyms.items():
            if original in skill_lower or skill_lower in original:
                return canonical
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        skill_clean = skill_lower.translate(str.maketrans('', '', string.punctuation))
        
        # è¿‡æ»¤æ³›åŒ–æœ¯è¯­ (åªè¿‡æ»¤å®Œå…¨åŒ¹é…)
        if skill_clean in self.generic_terms:
            return None
        
        # ä¿ç•™æœ‰æ„ä¹‰çš„æŠ€æœ¯æœ¯è¯­
        if len(skill_clean) >= 2:
            return skill_clean
        
        return None
    
    def extract_units_from_text(self, text: str) -> List[Tuple[str, str]]:
        """ä»æ–‡æœ¬ä¸­æå–å•å…ƒä»£ç å’Œåç§°"""
        units = []
        
        # æŸ¥æ‰¾å•å…ƒä»£ç 
        unit_codes = re.findall(self.unit_pattern, text)
        
        for code in set(unit_codes):
            # æŸ¥æ‰¾å¯¹åº”çš„å•å…ƒåç§°
            pattern = rf'{code}[^\n]*'
            matches = re.findall(pattern, text)
            
            if matches:
                full_text = matches[0]
                # æå–å•å…ƒåç§°
                name_part = re.sub(rf'^{code}\s*', '', full_text).strip()
                
                # æ¸…ç†åç§°
                name_part = re.sub(r'<[^>]*>', '', name_part)  # ç§»é™¤HTMLæ ‡ç­¾
                name_part = re.sub(r'\|.*$', '', name_part)    # ç§»é™¤è¡¨æ ¼åˆ†éš”ç¬¦åå†…å®¹
                name_part = name_part.strip()
                
                if name_part and len(name_part) > 3:
                    units.append((code, name_part[:50]))
                else:
                    units.append((code, f"Unit {code}"))
            else:
                units.append((code, f"Unit {code}"))
        
        return units
    
    def extract_skills_from_text(self, text: str, context: str = "general") -> Set[str]:
        """ä»æ–‡æœ¬ä¸­æå–æŠ€èƒ½å…³é”®è¯"""
        skills = set()
        text_lower = text.lower()
        
        # æŠ€æœ¯å…³é”®è¯æ¨¡å¼ (æ›´å®½æ¾)
        tech_patterns = [
            # ç¼–ç¨‹è¯­è¨€å’Œå·¥å…·
            r'\b(python|java|javascript|c\+\+|sql|html|css|react|angular|nodejs|tensorflow|pytorch|matlab|r)\b',
            # æŠ€æœ¯æ¦‚å¿µ
            r'\b(\w+)\s+(?:learning|recognition|detection|analysis|processing|development|programming|mining|vision)\b',
            # ä½¿ç”¨æŸæŠ€æœ¯
            r'\b(?:using|with|through|via|implement|develop|design|build|apply|utilize)\s+(\w+(?:\s+\w+){0,2})\b',
            # å­¦ä¹ ç›®æ ‡
            r'\b(?:learn|understand|master|acquire|develop)\s+(\w+(?:\s+\w+){0,2})\b',
            # æŠ€èƒ½æè¿°
            r'\b(\w+(?:\s+\w+){0,2})\s+(?:skills?|techniques?|methods?|approaches?)\b',
            # ä¸“ä¸šæœ¯è¯­
            r'\b(machine\s+learning|deep\s+learning|data\s+mining|computer\s+vision|natural\s+language|human\s+computer|web\s+development|mobile\s+development)\b',
            # ç¼©å†™
            r'\b(ai|ml|nlp|hci|ux|ui|iot|api|sql|html|css|js)\b'
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, text_lower)
            for match in matches:
                if isinstance(match, tuple):
                    for m in match:
                        if m:
                            normalized = self.normalize_skill(m)
                            if normalized:
                                skills.add(normalized)
                else:
                    if match:
                        normalized = self.normalize_skill(match)
                        if normalized:
                            skills.add(normalized)
        
        # ä»å•è¯ç»„åˆä¸­æå–æŠ€èƒ½
        words = re.findall(r'\b\w+\b', text_lower)
        for i in range(len(words)):
            # å•è¯
            skill = self.normalize_skill(words[i])
            if skill:
                skills.add(skill)
            
            # åŒè¯ç»„åˆ
            if i < len(words) - 1:
                phrase = f"{words[i]} {words[i+1]}"
                skill = self.normalize_skill(phrase)
                if skill:
                    skills.add(skill)
            
            # ä¸‰è¯ç»„åˆ
            if i < len(words) - 2:
                phrase = f"{words[i]} {words[i+1]} {words[i+2]}"
                skill = self.normalize_skill(phrase)
                if skill:
                    skills.add(skill)
        
        return skills
    
    def extract_project_skills(self, project_text: str) -> List[Tuple[str, float]]:
        """ä»é¡¹ç›®æè¿°ä¸­æå–æŠ€èƒ½"""
        
        # æå–æ‰€æœ‰æŠ€èƒ½
        skills = self.extract_skills_from_text(project_text, "project")
        
        # è®¡ç®—æŠ€èƒ½åˆ†æ•°
        text_lower = project_text.lower()
        skill_scores = []
        
        for skill in skills:
            score = 0
            skill_words = skill.split()
            
            # åŸºç¡€é¢‘ç‡åˆ†æ•°
            for word in skill_words:
                score += text_lower.count(word)
            
            # ä½ç½®åŠ æƒ (æ ‡é¢˜å’Œå¼€å¤´æ›´é‡è¦)
            first_200 = text_lower[:200]
            if any(word in first_200 for word in skill_words):
                score *= 2
            
            # å…³é”®è¯åŠ æƒ
            important_contexts = ['using', 'with', 'implement', 'develop', 'design', 'apply']
            for context in important_contexts:
                if any(f"{context} {word}" in text_lower for word in skill_words):
                    score += 2
            
            if score > 0:
                skill_scores.append((skill, score))
        
        # æ’åºå¹¶è¿”å›top-M
        skill_scores.sort(key=lambda x: x[1], reverse=True)
        return skill_scores[:self.M]
    
    def extract_unit_skills(self, unit_text: str, unit_code: str) -> List[Tuple[str, float]]:
        """ä»å•å…ƒå¤§çº²ä¸­æå–æŠ€èƒ½"""
        
        # æ‰©å±•çš„å•å…ƒæŠ€èƒ½æ˜ å°„
        unit_skill_mapping = {
            'IFN680': ['machine learning', 'data science', 'programming'],
            'IFN509': ['data science', 'machine learning', 'programming'],
            'IFN554': ['database', 'programming', 'data science'],
            'IFN666': ['web development', 'mobile development', 'programming'],
            'IFN507': ['networking', 'programming'],
            'IFN541': ['cybersecurity', 'networking'],
            'IFN557': ['web development', 'programming'],
            'IFN563': ['programming', 'software engineering'],
            'IFN555': ['programming'],
            'IFN591': ['user experience', 'web development'],
            'CAB432': ['networking', 'programming'],
            'CAB340': ['programming', 'machine learning'],
            'IFN619': ['data science', 'machine learning'],
            'IFN647': ['data science', 'web development'],
            'IFN648': ['cybersecurity', 'programming'],
            'IFN649': ['networking', 'cybersecurity'],
            'IFN644': ['networking', 'cybersecurity'],
            'IFN623': ['user experience', 'web development'],
            'IFN652': ['programming', 'data science'],
            'IFN657': ['cybersecurity', 'programming'],
            'IFN662': ['programming', 'data science'],
            'IFN664': ['programming', 'machine learning']
        }
        
        # è·å–é¢„å®šä¹‰æŠ€èƒ½
        predefined_skills = unit_skill_mapping.get(unit_code, [])
        
        # ä»æ–‡æœ¬ä¸­æå–æŠ€èƒ½
        extracted_skills = self.extract_skills_from_text(unit_text, "unit")
        
        # åˆå¹¶æŠ€èƒ½
        all_skills = set(predefined_skills) | extracted_skills
        
        # è®¡ç®—æŠ€èƒ½åˆ†æ•°
        skill_scores = []
        text_lower = unit_text.lower()
        
        for skill in all_skills:
            # åŸºç¡€åˆ†æ•°ï¼šé¢„å®šä¹‰æŠ€èƒ½åˆ†æ•°æ›´é«˜
            base_score = 5.0 if skill in predefined_skills else 1.0
            
            # æ–‡æœ¬é¢‘ç‡åˆ†æ•°
            skill_words = skill.split()
            frequency_score = 0
            for word in skill_words:
                frequency_score += text_lower.count(word)
            
            # å­¦ä¹ ç›®æ ‡åŠ æƒ
            learning_contexts = ['learn', 'understand', 'develop', 'design', 'implement', 'apply']
            for context in learning_contexts:
                if any(f"{context} {word}" in text_lower for word in skill_words):
                    frequency_score += 1
            
            total_score = base_score + frequency_score
            
            if total_score > 0:
                skill_scores.append((skill, total_score))
        
        # æ’åºå¹¶è¿”å›top-K
        skill_scores.sort(key=lambda x: x[1], reverse=True)
        return skill_scores[:self.K]
    
    def find_intersection_skills(self, project_skills: List[str], 
                               unit_skills_map: Dict[str, List[str]]) -> Set[str]:
        """æ‰¾å‡ºPDä¸UOçš„æŠ€èƒ½äº¤é›† (ä½¿ç”¨çµæ´»åŒ¹é…)"""
        project_skill_set = set(project_skills)
        
        all_unit_skills = set()
        for unit_skills in unit_skills_map.values():
            all_unit_skills.update(unit_skills)
        
        # ç›´æ¥äº¤é›†
        direct_intersection = project_skill_set & all_unit_skills
        
        # çµæ´»äº¤é›† (é€šè¿‡åŒä¹‰è¯)
        flexible_intersection = set()
        
        for p_skill in project_skill_set:
            for u_skill in all_unit_skills:
                # æ£€æŸ¥æ˜¯å¦æœ‰è¯æ±‡é‡å 
                p_words = set(p_skill.split())
                u_words = set(u_skill.split())
                
                if p_words & u_words:  # æœ‰å…±åŒè¯æ±‡
                    flexible_intersection.add(p_skill)
                    flexible_intersection.add(u_skill)
        
        # åˆå¹¶äº¤é›†
        total_intersection = direct_intersection | flexible_intersection
        
        return total_intersection
    
    def extract_clean_kg(self, project_file: str, unit_dir: str = "unit_md") -> Tuple[Dict, List[str]]:
        """æå–æ¸…æ´çŸ¥è¯†å›¾è°±"""
        
        # è¯»å–é¡¹ç›®æè¿°
        with open(project_file, 'r', encoding='utf-8') as f:
            project_content = f.read()
        
        project_name = self._extract_project_title(project_content)
        project_id = f"project_{os.path.splitext(os.path.basename(project_file))[0]}"
        
        # æå–é¡¹ç›®æŠ€èƒ½
        project_skills_scored = self.extract_project_skills(project_content)
        project_skills = [skill for skill, score in project_skills_scored]
        
        print(f"é¡¹ç›®æŠ€èƒ½ ({len(project_skills)}): {project_skills}")
        
        # å¤„ç†å•å…ƒå¤§çº²
        unit_data = {}
        unit_skills_map = {}
        
        if os.path.exists(unit_dir):
            unit_files = [f for f in os.listdir(unit_dir) if f.endswith('.md')]
            
            for unit_file in unit_files:
                unit_path = os.path.join(unit_dir, unit_file)
                with open(unit_path, 'r', encoding='utf-8') as f:
                    unit_content = f.read()
                
                units_in_file = self.extract_units_from_text(unit_content)
                
                for unit_code, unit_name in units_in_file:
                    unit_skills_scored = self.extract_unit_skills(unit_content, unit_code)
                    unit_skills = [skill for skill, score in unit_skills_scored]
                    
                    unit_data[unit_code] = {
                        'name': unit_name,
                        'skills': unit_skills,
                        'scores': dict(unit_skills_scored)
                    }
                    unit_skills_map[unit_code] = unit_skills
        
        print(f"å•å…ƒæ•°é‡: {len(unit_data)}")
        all_unit_skills = set()
        for skills in unit_skills_map.values():
            all_unit_skills.update(skills)
        print(f"å”¯ä¸€å•å…ƒæŠ€èƒ½: {len(all_unit_skills)}")
        print(f"ç¤ºä¾‹å•å…ƒæŠ€èƒ½: {list(all_unit_skills)[:10]}")
        
        # æ‰¾å‡ºäº¤é›†æŠ€èƒ½
        intersection_skills = self.find_intersection_skills(project_skills, unit_skills_map)
        print(f"äº¤é›†æŠ€èƒ½ ({len(intersection_skills)}): {list(intersection_skills)}")
        
        if not intersection_skills:
            print("âš ï¸ ä»ç„¶æ²¡æœ‰å‘ç°PDä¸UOçš„æŠ€èƒ½äº¤é›†")
            print("ğŸ” å°è¯•åˆ†æåŸå› ...")
            print(f"é¡¹ç›®æŠ€èƒ½ç¤ºä¾‹: {project_skills[:3]}")
            print(f"å•å…ƒæŠ€èƒ½ç¤ºä¾‹: {list(all_unit_skills)[:5]}")
            return {"nodes": [], "edges": []}, []
        
        # æ„å»ºçŸ¥è¯†å›¾è°±
        nodes = []
        edges = []
        triples = []
        
        # 1. æ·»åŠ é¡¹ç›®èŠ‚ç‚¹
        nodes.append({
            "id": project_id,
            "type": "PROJECT", 
            "name": project_name
        })
        
        # 2. æ·»åŠ äº¤é›†æŠ€èƒ½èŠ‚ç‚¹å’Œé¡¹ç›®â†’æŠ€èƒ½å…³ç³»
        project_skill_scores = dict(project_skills_scored)
        for skill in intersection_skills:
            skill_id = f"skill_{skill.replace(' ', '_')}"
            
            # æ·»åŠ æŠ€èƒ½èŠ‚ç‚¹
            nodes.append({
                "id": skill_id,
                "type": "SKILL",
                "name": skill
            })
            
            # æ·»åŠ é¡¹ç›®â†’æŠ€èƒ½å…³ç³»
            weight = project_skill_scores.get(skill, 1.0)
            edges.append({
                "source": project_id,
                "target": skill_id,
                "relation": "requires",
                "weight": weight
            })
            
            triples.append(f"{project_name} requires {skill}")
        
        # 3. æ·»åŠ å•å…ƒèŠ‚ç‚¹å’Œå…³ç³»
        programs_added = set()
        
        for unit_code, unit_info in unit_data.items():
            unit_id = f"unit_{unit_code}"
            unit_name = f"{unit_code} {unit_info['name']}"
            
            # æ£€æŸ¥è¯¥å•å…ƒæ˜¯å¦æ•™æˆäº¤é›†æŠ€èƒ½
            unit_intersection_skills = set(unit_info['skills']) & intersection_skills
            if not unit_intersection_skills:
                continue
            
            # æ·»åŠ å•å…ƒèŠ‚ç‚¹
            nodes.append({
                "id": unit_id,
                "type": "UNIT",
                "name": unit_name
            })
            
            # æ·»åŠ ç¨‹åºèŠ‚ç‚¹å’Œå•å…ƒâ†’ç¨‹åºå…³ç³»
            unit_prefix = unit_code[:3]
            program_name = self.program_mapping.get(unit_prefix, f"{unit_prefix} Program")
            program_id = f"program_{unit_prefix}"
            
            if program_id not in programs_added:
                nodes.append({
                    "id": program_id,
                    "type": "PROGRAM",
                    "name": program_name
                })
                programs_added.add(program_id)
            
            # å•å…ƒâ†’ç¨‹åºå…³ç³»
            edges.append({
                "source": unit_id,
                "target": program_id,
                "relation": "belongs_to", 
                "weight": 1.0
            })
            
            triples.append(f"{unit_name} belongs_to {program_name}")
            
            # æ·»åŠ å•å…ƒâ†’æŠ€èƒ½å…³ç³»
            unit_skill_scores = unit_info['scores']
            
            for skill in unit_intersection_skills:
                skill_id = f"skill_{skill.replace(' ', '_')}"
                weight = unit_skill_scores.get(skill, 1.0)
                
                edges.append({
                    "source": unit_id,
                    "target": skill_id,
                    "relation": "teaches",
                    "weight": weight
                })
                
                triples.append(f"{unit_name} teaches {skill}")
        
        # æ„å»ºæœ€ç»ˆå›¾ç»“æ„
        kg_data = {
            "nodes": nodes,
            "edges": edges
        }
        
        return kg_data, triples
    
    def _extract_project_title(self, content: str) -> str:
        """æå–é¡¹ç›®æ ‡é¢˜"""
        lines = content.split('\n')
        
        # æŸ¥æ‰¾è¡¨æ ¼æ ¼å¼æ ‡é¢˜
        for line in lines:
            if 'project title' in line.lower() and '|' in line:
                parts = line.split('|')
                if len(parts) >= 3:
                    title = parts[-2].strip()
                    if title and len(title) > 3:
                        return title
        
        # æŸ¥æ‰¾Markdownæ ‡é¢˜
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()
        
        return "Unknown Project"
    
    def export_triples(self, triples: List[str], output_file: str):
        """å¯¼å‡ºä¸‰å…ƒç»„æ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            for triple in triples:
                f.write(f"{triple}\n")
        print(f"âœ… ä¸‰å…ƒç»„å¯¼å‡º: {output_file}")
    
    def export_json(self, kg_data: Dict, output_file: str):
        """å¯¼å‡ºJSONæ ¼å¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(kg_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONå¯¼å‡º: {output_file}")
    
    def process_project(self, project_file: str, output_dir: str = None):
        """å¤„ç†å•ä¸ªé¡¹ç›®"""
        
        project_name = os.path.splitext(os.path.basename(project_file))[0]
        print(f"\nğŸ” å¤„ç†é¡¹ç›®: {project_name}")
        
        try:
            # æå–çŸ¥è¯†å›¾è°±
            kg_data, triples = self.extract_clean_kg(project_file)
            
            if not kg_data["nodes"]:
                print("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„çŸ¥è¯†å›¾è°±ï¼ˆæ— æŠ€èƒ½äº¤é›†ï¼‰")
                return False
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            if not output_dir:
                output_dir = f"flexible_clean_kg_output/{project_name}"
            os.makedirs(output_dir, exist_ok=True)
            
            # å¯¼å‡ºæ–‡ä»¶
            self.export_triples(triples, os.path.join(output_dir, f"{project_name}_triples.txt"))
            self.export_json(kg_data, os.path.join(output_dir, f"{project_name}_kg.json"))
            
            # ç”Ÿæˆç»Ÿè®¡
            stats = {
                "project": project_name,
                "nodes": len(kg_data["nodes"]),
                "edges": len(kg_data["edges"]),
                "triples": len(triples),
                "node_types": Counter(node["type"] for node in kg_data["nodes"]),
                "relation_types": Counter(edge["relation"] for edge in kg_data["edges"])
            }
            
            with open(os.path.join(output_dir, f"{project_name}_stats.json"), 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æˆåŠŸç”Ÿæˆ: {stats['nodes']} èŠ‚ç‚¹, {stats['edges']} è¾¹, {stats['triples']} ä¸‰å…ƒç»„")
            return True
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  çµæ´»æ¸…æ´çŸ¥è¯†å›¾è°±ä¿¡æ¯æå–ä»£ç†")
    print("=" * 60)
    print("ğŸ“‹ æ”¹è¿›: æ”¾å®½åŒ¹é…æ¡ä»¶ï¼Œå¢å¼ºäº¤é›†æ£€æµ‹")
    print("ğŸ¯ è¾“å‡º: ä¸‰å…ƒç»„ + JSON")
    print("=" * 60)
    
    extractor = FlexibleCleanKGExtractor()
    
    # å¤„ç†ç¤ºä¾‹é¡¹ç›®
    test_project = "project_md/HAR_WiFi_Proposal_Zhenguo-1.md"
    
    if os.path.exists(test_project):
        extractor.process_project(test_project)
    else:
        print(f"âŒ ç¤ºä¾‹é¡¹ç›®æ–‡ä»¶ä¸å­˜åœ¨: {test_project}")
    
    print("\nğŸ‰ å¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    main()
