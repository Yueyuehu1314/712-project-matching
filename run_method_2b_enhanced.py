#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Method 2b Enhanced: KG Similarity with Gap Analysis

å¢å¼ºç‰ˆMethod 2bå®éªŒï¼ŒåŒ…å«ï¼š
1. èŠ‚ç‚¹å’Œè¾¹çš„Jaccardç›¸ä¼¼åº¦
2. è¯¦ç»†çš„gapåˆ†æï¼ˆå­¦ç”Ÿç¼ºå°‘ä»€ä¹ˆï¼‰
3. å­¦ä¹ å»ºè®®ç”Ÿæˆ
"""

import os
import json
import glob
from pathlib import Path
from typing import Dict, List, Set
import numpy as np
from collections import defaultdict

# å¯¼å…¥åŸæœ‰çš„ç±»
from run_kg_similarity_experiment import (
    KnowledgeGraphLoader,
    GraphSimilarityComparator,
    GraphSimilarityScore
)


class GapAnalyzer:
    """å­¦ç”Ÿ-é¡¹ç›®çŸ¥è¯†å·®è·åˆ†æå™¨"""
    
    def __init__(self):
        self.loader = KnowledgeGraphLoader()
    
    def analyze_gap(self, project_kg: Dict, student_kg: Dict, 
                    project_name: str, student_id: str) -> Dict:
        """åˆ†æå­¦ç”Ÿç›¸å¯¹äºé¡¹ç›®çš„çŸ¥è¯†å·®è·"""
        
        # æå–èŠ‚ç‚¹å’Œè¾¹
        project_nodes = self.loader.extract_node_ids(project_kg)
        student_nodes = self.loader.extract_node_ids(student_kg)
        project_edges = self.loader.extract_edges(project_kg)
        student_edges = self.loader.extract_edges(student_kg)
        
        # è·å–å®ä½“è¯¦æƒ…
        project_entities = self._get_entity_details(project_kg)
        student_entities = self._get_entity_details(student_kg)
        
        # è®¡ç®—ç¼ºå¤±çš„èŠ‚ç‚¹
        missing_nodes = project_nodes - student_nodes
        missing_node_details = [
            project_entities.get(node_id, {'id': node_id, 'name': node_id})
            for node_id in missing_nodes
        ]
        
        # è®¡ç®—ç¼ºå¤±çš„è¾¹ï¼ˆåªè€ƒè™‘å…±åŒèŠ‚ç‚¹ä¹‹é—´çš„è¾¹ï¼‰
        common_nodes = project_nodes & student_nodes
        if common_nodes:
            project_edges_filtered = {(s, t) for s, t in project_edges 
                                     if s in common_nodes and t in common_nodes}
            student_edges_filtered = {(s, t) for s, t in student_edges 
                                     if s in common_nodes and t in common_nodes}
            missing_edges = project_edges_filtered - student_edges_filtered
        else:
            missing_edges = set()
        
        # è®¡ç®—modification steps
        node_steps = len(missing_nodes)
        edge_steps = len(missing_edges)
        total_steps = node_steps + edge_steps
        
        return {
            'project_name': project_name,
            'student_id': student_id,
            'total_modification_steps': total_steps,
            'missing_nodes_count': node_steps,
            'missing_edges_count': edge_steps,
            'missing_nodes': sorted([
                {
                    'id': entity.get('id', ''),
                    'name': entity.get('name', ''),
                    'type': entity.get('type', 'Unknown')
                }
                for entity in missing_node_details
            ], key=lambda x: x['type']),
            'missing_edges': [
                {
                    'source': s,
                    'target': t,
                    'source_name': project_entities.get(s, {}).get('name', s),
                    'target_name': project_entities.get(t, {}).get('name', t)
                }
                for s, t in sorted(missing_edges)
            ],
            'common_nodes_count': len(common_nodes),
            'readiness_score': len(common_nodes) / len(project_nodes) if project_nodes else 0.0
        }
    
    def _get_entity_details(self, kg_data: Dict) -> Dict:
        """æå–å®ä½“è¯¦æƒ…"""
        entities = {}
        
        if isinstance(kg_data, dict):
            if 'entities' in kg_data:
                for entity in kg_data['entities']:
                    if isinstance(entity, dict) and 'id' in entity:
                        entities[entity['id']] = entity
        elif isinstance(kg_data, list):
            for entity in kg_data:
                if isinstance(entity, dict) and 'id' in entity:
                    entities[entity['id']] = entity
        
        return entities
    
    def generate_learning_recommendations(self, gap_analysis: Dict) -> List[str]:
        """åŸºäºgapåˆ†æç”Ÿæˆå­¦ä¹ å»ºè®®"""
        recommendations = []
        
        missing_nodes = gap_analysis['missing_nodes']
        missing_edges = gap_analysis['missing_edges']
        total_steps = gap_analysis['total_modification_steps']
        readiness = gap_analysis['readiness_score']
        
        # æ€»ä½“è¯„ä¼°
        if readiness >= 0.8:
            recommendations.append("âœ… é«˜åº¦åŒ¹é…ï¼å­¦ç”Ÿå·²æŒæ¡é¡¹ç›®æ‰€éœ€çš„å¤§éƒ¨åˆ†çŸ¥è¯†ã€‚")
        elif readiness >= 0.5:
            recommendations.append("âš ï¸ ä¸­ç­‰åŒ¹é…ã€‚å­¦ç”Ÿéœ€è¦è¡¥å……ä¸€äº›å…³é”®æŠ€èƒ½ã€‚")
        else:
            recommendations.append("âŒ åŒ¹é…åº¦è¾ƒä½ã€‚å­¦ç”Ÿéœ€è¦å¤§é‡å­¦ä¹ æ‰èƒ½èƒœä»»æ­¤é¡¹ç›®ã€‚")
        
        recommendations.append(f"\nğŸ“Š éœ€è¦ {total_steps} ä¸ªå­¦ä¹ æ­¥éª¤ ({gap_analysis['missing_nodes_count']} ä¸ªæŠ€èƒ½ + {gap_analysis['missing_edges_count']} ä¸ªçŸ¥è¯†å…³ç³»)")
        
        # æŒ‰ç±»å‹åˆ†ç»„ç¼ºå¤±çš„æŠ€èƒ½
        skills_by_type = defaultdict(list)
        for node in missing_nodes:
            node_type = node.get('type', 'Unknown')
            node_name = node.get('name', node.get('id', ''))
            skills_by_type[node_type].append(node_name)
        
        if skills_by_type:
            recommendations.append("\nğŸ“š éœ€è¦å­¦ä¹ çš„æŠ€èƒ½ï¼š")
            for skill_type, skills in sorted(skills_by_type.items()):
                if skills:
                    recommendations.append(f"\n  {skill_type}:")
                    for skill in sorted(skills)[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                        recommendations.append(f"    â€¢ {skill}")
                    if len(skills) > 5:
                        recommendations.append(f"    ... ä»¥åŠå…¶ä»– {len(skills) - 5} ä¸ª")
        
        # æ¨èè¯¾ç¨‹
        if missing_nodes:
            recommendations.append("\nğŸ“ å»ºè®®ä¿®è¯»çš„è¯¾ç¨‹ï¼š")
            # è¿™é‡Œå¯ä»¥æ ¹æ®ç¼ºå¤±çš„æŠ€èƒ½æ¨èå…·ä½“è¯¾ç¨‹
            # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæŠ€èƒ½ç±»å‹æ¨è
            skill_types = set(node.get('type', '') for node in missing_nodes)
            if 'Technology' in skill_types or 'Tool' in skill_types:
                recommendations.append("    â€¢ ç›¸å…³æŠ€æœ¯è¯¾ç¨‹ï¼ˆå¦‚ç¼–ç¨‹ã€æ¡†æ¶ä½¿ç”¨ï¼‰")
            if 'Domain' in skill_types or 'Concept' in skill_types:
                recommendations.append("    â€¢ ä¸“ä¸šé¢†åŸŸè¯¾ç¨‹ï¼ˆå¦‚æœºå™¨å­¦ä¹ ã€æ•°æ®ç§‘å­¦ï¼‰")
            if 'Method' in skill_types:
                recommendations.append("    â€¢ æ–¹æ³•è®ºè¯¾ç¨‹ï¼ˆå¦‚ç ”ç©¶æ–¹æ³•ã€ç®—æ³•è®¾è®¡ï¼‰")
        
        return recommendations


def load_project_name_mapping(mapping_file: str) -> Dict[str, str]:
    """
    åŠ è½½é¡¹ç›®åç§°æ˜ å°„æ–‡ä»¶
    
    Returns:
        Dict[ç®€åŒ–åç§° -> åŸå§‹é¡¹ç›®ç›®å½•å]
    """
    if not os.path.exists(mapping_file):
        print(f"âš ï¸  æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_file}")
        return {}
    
    with open(mapping_file, 'r', encoding='utf-8') as f:
        mapping = json.load(f)
    
    print(f"âœ“ åŠ è½½é¡¹ç›®åç§°æ˜ å°„: {len(mapping)} ä¸ªé¡¹ç›®")
    return mapping


def create_full_title_to_student_dir_mapping(
    project_kg_dir: str, 
    student_kg_dir: str,
    mapping: Dict[str, str]
) -> Dict[str, str]:
    """
    åˆ›å»ºå®Œæ•´é¡¹ç›®æ ‡é¢˜åˆ°å­¦ç”ŸKGç›®å½•çš„æ˜ å°„
    
    Args:
        project_kg_dir: é¡¹ç›®KGç›®å½•ï¼ˆenhanced_in20_in27ï¼‰
        student_kg_dir: å­¦ç”ŸKGç›®å½•ï¼ˆenhanced_student_kgï¼‰
        mapping: ç®€åŒ–åç§°åˆ°åŸå§‹ç›®å½•çš„æ˜ å°„
    
    Returns:
        Dict[å®Œæ•´é¡¹ç›®ç›®å½•å -> å­¦ç”ŸKGç›®å½•å]
    """
    title_mapping = {}
    
    # è·å–æ‰€æœ‰å­¦ç”ŸKGç›®å½•åç§°
    student_dirs = {d.name for d in Path(student_kg_dir).iterdir() if d.is_dir()}
    
    # éå†é¡¹ç›®KGç›®å½•
    for proj_dir in Path(project_kg_dir).iterdir():
        if not proj_dir.is_dir():
            continue
        
        full_title = proj_dir.name
        
        # æ–¹æ³•1: ç›´æ¥åŒ¹é…ï¼ˆé¡¹ç›®åç§°ç›¸åŒï¼‰
        if full_title in student_dirs:
            title_mapping[full_title] = full_title
            print(f"  âœ“ ç›´æ¥åŒ¹é…: {full_title}")
            continue
        
        # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„çš„å€¼ä¸­ï¼ˆåŸå§‹é¡¹ç›®åï¼‰
        if full_title in mapping.values():
            title_mapping[full_title] = full_title
            print(f"  âœ“ æ˜ å°„å€¼åŒ¹é…: {full_title}")
            continue
        
        # æ–¹æ³•3: é€šè¿‡ç®€åŒ–åç§°çš„æ˜ å°„åŒ¹é…
        matched = False
        best_match = None
        best_score = 0
        best_simplified = None
        
        for simplified, original in mapping.items():
            # æ£€æŸ¥ç®€åŒ–åç§°çš„ä¸»è¦éƒ¨åˆ†æ˜¯å¦åœ¨å®Œæ•´æ ‡é¢˜ä¸­
            simplified_parts = simplified.replace('_', ' ').replace('-', ' ').split()
            
            # è®¡ç®—åŒ¹é…åˆ†æ•°
            matches = sum(1 for part in simplified_parts 
                        if len(part) > 2 and part.lower() in full_title.lower())
            
            # å¦‚æœåŒ¹é…åˆ†æ•°æ›´å¥½
            if matches > best_score:
                best_score = matches
                best_match = original
                best_simplified = simplified
        
        # å¦‚æœè‡³å°‘æœ‰2ä¸ªå…³é”®è¯åŒ¹é…
        if best_simplified and best_score >= 2:
            title_mapping[full_title] = best_match
            matched = True
            print(f"  âœ“ å…³é”®è¯åŒ¹é…: {full_title[:50]}... -> {best_match}")
        
        if not matched:
            # æ–¹æ³•4: å°è¯•ç›´æ¥åœ¨å­¦ç”Ÿç›®å½•ä¸­æŸ¥æ‰¾åŒ…å«ç›¸ä¼¼å…³é”®è¯çš„ç›®å½•
            title_keywords = set(w.lower() for w in full_title.replace('_', ' ').split() if len(w) > 3)
            for student_dir in student_dirs:
                student_keywords = set(w.lower() for w in student_dir.replace('_', ' ').split() if len(w) > 3)
                common = title_keywords & student_keywords
                if len(common) >= 3:
                    title_mapping[full_title] = student_dir
                    print(f"  âœ“ å…³é”®è¯æœç´¢: {full_title[:50]}... -> {student_dir}")
                    matched = True
                    break
        
        if not matched:
            print(f"  âš ï¸  æœªæ‰¾åˆ°æ˜ å°„: {full_title}")
    
    return title_mapping


def run_method_2b_with_gap_analysis():
    """è¿è¡ŒMethod 2bå¹¶ç”Ÿæˆgapåˆ†æ"""
    
    print("=" * 80)
    print("Method 2b Enhanced: KG Similarity + Gap Analysis")
    print("=" * 80)
    print()
    
    # é…ç½®è·¯å¾„
    project_kg_dir = "outputs1/knowledge_graphs/enhanced_in20_in27"
    student_kg_dir = "outputs1/knowledge_graphs/enhanced_student_kg"
    mapping_file = "outputs1/knowledge_graphs/project_name_mapping.json"
    output_dir = Path("outputs/kg_similarity")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é¡¹ç›®åç§°æ˜ å°„
    print("ğŸ“‹ åŠ è½½é¡¹ç›®åç§°æ˜ å°„...")
    name_mapping = load_project_name_mapping(mapping_file)
    
    # åˆ›å»ºå®Œæ•´æ ‡é¢˜åˆ°å­¦ç”Ÿç›®å½•çš„æ˜ å°„
    print("\nğŸ”— å»ºç«‹é¡¹ç›®æ ‡é¢˜æ˜ å°„...")
    title_to_student_dir = create_full_title_to_student_dir_mapping(
        project_kg_dir, student_kg_dir, name_mapping
    )
    
    print(f"\nâœ“ æˆåŠŸæ˜ å°„: {len(title_to_student_dir)} ä¸ªé¡¹ç›®")
    print()
    
    # åˆå§‹åŒ–
    loader = KnowledgeGraphLoader()
    comparator = GraphSimilarityComparator()
    gap_analyzer = GapAnalyzer()
    
    # è·å–æ‰€æœ‰é¡¹ç›®ç›®å½•
    project_dirs = [d for d in Path(project_kg_dir).iterdir() if d.is_dir()]
    
    if not project_dirs:
        print("âŒ æœªæ‰¾åˆ°é¡¹ç›®KGç›®å½•")
        return
    
    print(f"æ‰¾åˆ° {len(project_dirs)} ä¸ªé¡¹ç›®ç›®å½•")
    print()
    
    # å­˜å‚¨ç»“æœ
    all_scores = []
    all_gaps = []
    
    # å¯¹æ¯ä¸ªé¡¹ç›®
    for proj_dir in sorted(project_dirs):
        proj_name = proj_dir.name
        
        print(f"å¤„ç†é¡¹ç›®: {proj_name}")
        
        # åŠ è½½é¡¹ç›®KG
        kg_files = list(proj_dir.glob("*_enhanced_kg.json"))
        if not kg_files:
            print(f"  âš ï¸  æœªæ‰¾åˆ°KGæ–‡ä»¶")
            continue
        
        project_kg = loader.load_kg_json(str(kg_files[0]))
        
        # æŸ¥æ‰¾å¯¹åº”çš„å­¦ç”ŸKGç›®å½•
        student_dir_name = title_to_student_dir.get(proj_name)
        if not student_dir_name:
            print(f"  âš ï¸  æœªæ‰¾åˆ°å­¦ç”ŸKGç›®å½•æ˜ å°„")
            continue
        
        # æ‰¾åˆ°è¯¥é¡¹ç›®çš„å­¦ç”ŸKG
        student_kg_path = Path(student_kg_dir) / student_dir_name
        if not student_kg_path.exists():
            print(f"  âš ï¸  å­¦ç”ŸKGç›®å½•ä¸å­˜åœ¨: {student_dir_name}")
            continue
        
        student_files = list(student_kg_path.glob("*_enhanced_kg.json"))
        
        if not student_files:
            print(f"  âš ï¸  æœªæ‰¾åˆ°å­¦ç”ŸKGæ–‡ä»¶")
            continue
        
        # å¯¹æ¯ä¸ªå­¦ç”Ÿ
        for student_file in student_files:
            student_id = Path(student_file).stem
            student_kg = loader.load_kg_json(student_file)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            score = comparator.compare_graphs(
                project_kg, student_kg,
                proj_name, student_id, is_match=True
            )
            all_scores.append(score)
            
            # åˆ†ægap
            gap = gap_analyzer.analyze_gap(
                project_kg, student_kg,
                proj_name, student_id
            )
            all_gaps.append(gap)
        
        print(f"  âœ“ å·²å¤„ç† {len(student_files)} ä¸ªå­¦ç”Ÿ")
    
    print()
    print(f"âœ“ æ€»å…±å¤„ç†äº† {len(all_scores)} ä¸ªé¡¹ç›®-å­¦ç”Ÿå¯¹")
    print()
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    
    # 1. ç›¸ä¼¼åº¦åˆ†æ•°
    scores_file = output_dir / "method_2b_scores_enhanced.json"
    with open(scores_file, 'w', encoding='utf-8') as f:
        json.dump([vars(s) for s in all_scores], f, indent=2, ensure_ascii=False)
    print(f"   - {scores_file}")
    
    # 2. Gapåˆ†æ
    gaps_file = output_dir / "method_2b_gaps.json"
    with open(gaps_file, 'w', encoding='utf-8') as f:
        json.dump(all_gaps, f, indent=2, ensure_ascii=False)
    print(f"   - {gaps_file}")
    
    # 3. ç»Ÿè®¡åˆ†æ
    analysis = analyze_results(all_scores, all_gaps)
    analysis_file = output_dir / "method_2b_analysis_enhanced.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    print(f"   - {analysis_file}")
    
    # 4. ç”Ÿæˆå­¦ä¹ å»ºè®®æŠ¥å‘Š
    generate_learning_report(all_gaps, gap_analyzer, output_dir)
    
    print()
    print("âœ… å®éªŒå®Œæˆï¼")
    
    return all_scores, all_gaps, analysis


def analyze_results(scores: List[GraphSimilarityScore], gaps: List[Dict]) -> Dict:
    """åˆ†æç»“æœ"""
    
    # ç›¸ä¼¼åº¦ç»Ÿè®¡
    jaccard_nodes = [s.jaccard_similarity for s in scores]
    jaccard_edges = [s.jaccard_edge_similarity for s in scores]
    edit_distances = [s.edit_distance for s in scores]
    
    # Gapç»Ÿè®¡
    modification_steps = [g['total_modification_steps'] for g in gaps]
    readiness_scores = [g['readiness_score'] for g in gaps]
    missing_nodes_counts = [g['missing_nodes_count'] for g in gaps]
    missing_edges_counts = [g['missing_edges_count'] for g in gaps]
    
    return {
        'method': 'method_2b_enhanced',
        'total_pairs': len(scores),
        'similarity_metrics': {
            'jaccard_nodes': {
                'mean': float(np.mean(jaccard_nodes)),
                'median': float(np.median(jaccard_nodes)),
                'std': float(np.std(jaccard_nodes)),
                'min': float(np.min(jaccard_nodes)),
                'max': float(np.max(jaccard_nodes))
            },
            'jaccard_edges': {
                'mean': float(np.mean(jaccard_edges)),
                'median': float(np.median(jaccard_edges)),
                'std': float(np.std(jaccard_edges)),
                'min': float(np.min(jaccard_edges)),
                'max': float(np.max(jaccard_edges))
            },
            'edit_distance': {
                'mean': float(np.mean(edit_distances)),
                'median': float(np.median(edit_distances)),
                'std': float(np.std(edit_distances)),
                'min': float(np.min(edit_distances)),
                'max': float(np.max(edit_distances))
            }
        },
        'gap_analysis': {
            'modification_steps': {
                'mean': float(np.mean(modification_steps)),
                'median': float(np.median(modification_steps)),
                'std': float(np.std(modification_steps)),
                'min': float(np.min(modification_steps)),
                'max': float(np.max(modification_steps))
            },
            'readiness_score': {
                'mean': float(np.mean(readiness_scores)),
                'median': float(np.median(readiness_scores)),
                'std': float(np.std(readiness_scores)),
                'min': float(np.min(readiness_scores)),
                'max': float(np.max(readiness_scores))
            },
            'missing_nodes': {
                'mean': float(np.mean(missing_nodes_counts)),
                'median': float(np.median(missing_nodes_counts)),
                'std': float(np.std(missing_nodes_counts)),
                'min': float(np.min(missing_nodes_counts)),
                'max': float(np.max(missing_nodes_counts))
            },
            'missing_edges': {
                'mean': float(np.mean(missing_edges_counts)),
                'median': float(np.median(missing_edges_counts)),
                'std': float(np.std(missing_edges_counts)),
                'min': float(np.min(missing_edges_counts)),
                'max': float(np.max(missing_edges_counts))
            }
        }
    }


def generate_learning_report(gaps: List[Dict], gap_analyzer: GapAnalyzer, output_dir: Path):
    """ç”Ÿæˆå­¦ä¹ å»ºè®®æŠ¥å‘Š"""
    
    report_file = output_dir / "method_2b_learning_recommendations.txt"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("Method 2b: å­¦ç”Ÿå­¦ä¹ å»ºè®®æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # æŒ‰é¡¹ç›®åˆ†ç»„
        gaps_by_project = defaultdict(list)
        for gap in gaps:
            gaps_by_project[gap['project_name']].append(gap)
        
        # ä¸ºæ¯ä¸ªé¡¹ç›®ç”ŸæˆæŠ¥å‘Š
        for project_name in sorted(gaps_by_project.keys()):
            project_gaps = gaps_by_project[project_name]
            
            f.write(f"\n{'=' * 80}\n")
            f.write(f"é¡¹ç›®: {project_name}\n")
            f.write(f"{'=' * 80}\n\n")
            
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„å­¦ç”Ÿ
            best_gap = min(project_gaps, key=lambda x: x['total_modification_steps'])
            
            f.write(f"ğŸ“Š ç»Ÿè®¡æ‘˜è¦:\n")
            f.write(f"  - æ€»å­¦ç”Ÿæ•°: {len(project_gaps)}\n")
            f.write(f"  - å¹³å‡modification steps: {np.mean([g['total_modification_steps'] for g in project_gaps]):.1f}\n")
            f.write(f"  - å¹³å‡readiness: {np.mean([g['readiness_score'] for g in project_gaps]):.2%}\n")
            f.write(f"  - æœ€ä½³åŒ¹é…å­¦ç”Ÿ: {best_gap['student_id']} (steps={best_gap['total_modification_steps']})\n\n")
            
            # ä¸ºå‰3åå­¦ç”Ÿç”Ÿæˆè¯¦ç»†å»ºè®®
            top_students = sorted(project_gaps, key=lambda x: x['total_modification_steps'])[:3]
            
            f.write(f"ğŸ¯ Top 3 æœ€åŒ¹é…å­¦ç”Ÿçš„å­¦ä¹ å»ºè®®:\n\n")
            
            for i, gap in enumerate(top_students, 1):
                f.write(f"{'-' * 80}\n")
                f.write(f"ç¬¬ {i} å: {gap['student_id']}\n")
                f.write(f"{'-' * 80}\n\n")
                
                recommendations = gap_analyzer.generate_learning_recommendations(gap)
                for rec in recommendations:
                    f.write(rec + "\n")
                
                f.write("\n")
    
    print(f"   - {report_file}")


if __name__ == "__main__":
    scores, gaps, analysis = run_method_2b_with_gap_analysis()

