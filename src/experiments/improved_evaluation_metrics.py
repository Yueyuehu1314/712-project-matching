#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„è¯„ä¼°æŒ‡æ ‡
ä½¿ç”¨Top-Kå‡†ç¡®ç‡ã€æ’åç›¸å…³æ€§ç­‰æ›´å®é™…çš„æŒ‡æ ‡é‡æ–°è¯„ä¼°åŒ¹é…æ•ˆæœ
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import argparse


class ImprovedEvaluator:
    """æ”¹è¿›çš„è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.results = {}
    
    def load_similarity_matrix(self, method: str) -> Dict[str, List[Tuple[str, float]]]:
        """
        åŠ è½½ç›¸ä¼¼åº¦çŸ©é˜µå¹¶è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        è¿”å›: {student_id: [(project_id, similarity_score), ...]}
        """
        # æ ¹æ®æ–¹æ³•ç¡®å®šæ–‡ä»¶è·¯å¾„
        if method == "1a":
            result_file = Path("outputs/embeddings/1a/similarity_comparison_results.json")
        elif method == "1b":
            result_file = Path("outputs/embeddings/1b/similarity_comparison_results.json")
        elif method == "2a":
            result_file = Path("outputs/kg_similarity/2a/method_2a_scores.json")
        elif method == "2b":
            result_file = Path("outputs/kg_similarity/2b/method_2b_scores_enhanced.json")
        else:
            raise ValueError(f"Unknown method: {method}")
        
        if not result_file.exists():
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
            return {}
        
        with open(result_file, 'r') as f:
            data = json.load(f)
        
        # è§£ææ•°æ®ç»“æ„ï¼ˆéœ€è¦æ ¹æ®å®é™…æ ¼å¼è°ƒæ•´ï¼‰
        rankings = defaultdict(list)
        
        if method in ["1a", "1b"]:
            # Embeddingæ–¹æ³•çš„ç»“æœæ ¼å¼
            # éœ€è¦ä»raw_similaritiesæˆ–å…¶ä»–å­—æ®µæ„å»º
            # è¿™é‡Œæä¾›ä¸€ä¸ªæ¡†æ¶
            pass
        elif method in ["2a", "2b"]:
            # KGæ–¹æ³•çš„ç»“æœæ ¼å¼
            if isinstance(data, list):
                for item in data:
                    student_id = item.get("student_id", "")
                    project_id = item.get("project_id", "")
                    
                    # ä½¿ç”¨node_jaccardä½œä¸ºç›¸ä¼¼åº¦
                    similarity = item.get("node_jaccard", item.get("jaccard_similarity", 0))
                    
                    rankings[student_id].append((project_id, similarity))
        
        # å¯¹æ¯ä¸ªå­¦ç”Ÿçš„é¡¹ç›®æŒ‰ç›¸ä¼¼åº¦æ’åº
        for student_id in rankings:
            rankings[student_id].sort(key=lambda x: x[1], reverse=True)
        
        return dict(rankings)
    
    def create_ground_truth(self, rankings: Dict[str, List[Tuple[str, float]]]) -> Dict[str, str]:
        """
        åˆ›å»ºground truthï¼šæ¯ä¸ªå­¦ç”Ÿçš„çœŸå®åŒ¹é…é¡¹ç›®
        å‡è®¾ï¼šä¸ºæ¯ä¸ªå­¦ç”Ÿç”Ÿæˆæ¡£æ¡ˆæ—¶å°±æŒ‡å®šäº†ç›®æ ‡é¡¹ç›®
        """
        ground_truth = {}
        
        # ä»å­¦ç”ŸIDä¸­æå–é¡¹ç›®ä¿¡æ¯
        # ä¾‹å¦‚: "student_project0_0" -> project0
        for student_id in rankings.keys():
            # å°è¯•ä»IDä¸­æå–é¡¹ç›®ç¼–å·
            if "_" in student_id:
                parts = student_id.split("_")
                if len(parts) >= 2:
                    project_part = parts[1]  # project0, project1, etc.
                    ground_truth[student_id] = project_part
        
        return ground_truth
    
    def evaluate_topk_accuracy(
        self, 
        rankings: Dict[str, List[Tuple[str, float]]], 
        ground_truth: Dict[str, str],
        k_values: List[int] = [1, 3, 5, 10, 20]
    ) -> Dict[str, float]:
        """
        è®¡ç®—Top-Kå‡†ç¡®ç‡
        """
        results = {}
        
        for k in k_values:
            correct = 0
            total = 0
            
            for student_id, true_project in ground_truth.items():
                if student_id not in rankings:
                    continue
                
                # è·å–Top-Kæ¨è
                top_k = rankings[student_id][:k]
                top_k_projects = [proj_id for proj_id, score in top_k]
                
                # æ£€æŸ¥çœŸå®é¡¹ç›®æ˜¯å¦åœ¨Top-Kä¸­
                if true_project in top_k_projects:
                    correct += 1
                
                total += 1
            
            accuracy = correct / total if total > 0 else 0
            results[f"top_{k}_accuracy"] = accuracy
        
        return results
    
    def evaluate_ranking_quality(
        self, 
        rankings: Dict[str, List[Tuple[str, float]]], 
        ground_truth: Dict[str, str]
    ) -> Dict[str, float]:
        """
        è¯„ä¼°æ’åè´¨é‡
        """
        ranks = []
        reciprocal_ranks = []
        
        for student_id, true_project in ground_truth.items():
            if student_id not in rankings:
                continue
            
            ranking = rankings[student_id]
            project_ids = [proj_id for proj_id, score in ranking]
            
            try:
                # æ‰¾åˆ°çœŸå®é¡¹ç›®çš„æ’åï¼ˆ1-basedï¼‰
                rank = project_ids.index(true_project) + 1
                ranks.append(rank)
                reciprocal_ranks.append(1.0 / rank)
            except ValueError:
                # å¦‚æœçœŸå®é¡¹ç›®ä¸åœ¨æ’åä¸­ï¼Œç»™æœ€å·®æ’å
                ranks.append(len(project_ids) + 1)
                reciprocal_ranks.append(0)
        
        results = {
            "mean_rank": float(np.mean(ranks)),
            "median_rank": float(np.median(ranks)),
            "std_rank": float(np.std(ranks)),
            "mrr": float(np.mean(reciprocal_ranks)),  # Mean Reciprocal Rank
            "best_rank": int(np.min(ranks)),
            "worst_rank": int(np.max(ranks)),
        }
        
        return results
    
    def evaluate_score_distribution(
        self, 
        rankings: Dict[str, List[Tuple[str, float]]], 
        ground_truth: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        åˆ†æç›¸ä¼¼åº¦å¾—åˆ†çš„åˆ†å¸ƒ
        """
        matched_scores = []
        unmatched_scores = []
        
        for student_id, true_project in ground_truth.items():
            if student_id not in rankings:
                continue
            
            for project_id, score in rankings[student_id]:
                if project_id == true_project:
                    matched_scores.append(score)
                else:
                    unmatched_scores.append(score)
        
        results = {
            "matched": {
                "mean": float(np.mean(matched_scores)) if matched_scores else 0,
                "std": float(np.std(matched_scores)) if matched_scores else 0,
                "median": float(np.median(matched_scores)) if matched_scores else 0,
                "min": float(np.min(matched_scores)) if matched_scores else 0,
                "max": float(np.max(matched_scores)) if matched_scores else 0,
            },
            "unmatched": {
                "mean": float(np.mean(unmatched_scores)) if unmatched_scores else 0,
                "std": float(np.std(unmatched_scores)) if unmatched_scores else 0,
                "median": float(np.median(unmatched_scores)) if unmatched_scores else 0,
                "min": float(np.min(unmatched_scores)) if unmatched_scores else 0,
                "max": float(np.max(unmatched_scores)) if unmatched_scores else 0,
            },
        }
        
        # è®¡ç®—Cohen's d
        if matched_scores and unmatched_scores:
            pooled_std = np.sqrt((np.var(matched_scores) + np.var(unmatched_scores)) / 2)
            cohens_d = (np.mean(matched_scores) - np.mean(unmatched_scores)) / pooled_std if pooled_std > 0 else 0
            results["cohens_d"] = float(cohens_d)
        else:
            results["cohens_d"] = 0
        
        return results
    
    def evaluate_method(self, method: str) -> Dict[str, Any]:
        """
        å®Œæ•´è¯„ä¼°ä¸€ä¸ªæ–¹æ³•
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼° Method {method}")
        print(f"{'='*60}\n")
        
        # åŠ è½½æ•°æ®
        rankings = self.load_similarity_matrix(method)
        if not rankings:
            print(f"âš ï¸  æ— æ³•åŠ è½½ Method {method} çš„æ•°æ®")
            return {}
        
        # åˆ›å»ºground truth
        ground_truth = self.create_ground_truth(rankings)
        if not ground_truth:
            print("âš ï¸  æ— æ³•åˆ›å»ºground truth")
            return {}
        
        print(f"âœ… åŠ è½½äº† {len(rankings)} ä¸ªå­¦ç”Ÿçš„æ’å")
        print(f"âœ… Ground truthåŒ…å« {len(ground_truth)} ä¸ªåŒ¹é…\n")
        
        # å„ç§è¯„ä¼°
        topk_results = self.evaluate_topk_accuracy(rankings, ground_truth)
        ranking_results = self.evaluate_ranking_quality(rankings, ground_truth)
        score_dist = self.evaluate_score_distribution(rankings, ground_truth)
        
        # æ‰“å°ç»“æœ
        print("ğŸ“ˆ Top-Kå‡†ç¡®ç‡:")
        for k_name, accuracy in sorted(topk_results.items()):
            print(f"  {k_name}: {accuracy:.2%}")
        
        print("\nğŸ“Š æ’åè´¨é‡:")
        print(f"  å¹³å‡æ’å: {ranking_results['mean_rank']:.2f}")
        print(f"  ä¸­ä½æ•°æ’å: {ranking_results['median_rank']:.2f}")
        print(f"  MRR: {ranking_results['mrr']:.4f}")
        print(f"  æ’åèŒƒå›´: [{ranking_results['best_rank']}, {ranking_results['worst_rank']}]")
        
        print("\nğŸ“‰ å¾—åˆ†åˆ†å¸ƒ:")
        print(f"  Matchedå¾—åˆ†: {score_dist['matched']['mean']:.4f} Â± {score_dist['matched']['std']:.4f}")
        print(f"  Unmatchedå¾—åˆ†: {score_dist['unmatched']['mean']:.4f} Â± {score_dist['unmatched']['std']:.4f}")
        print(f"  Cohen's d: {score_dist['cohens_d']:.4f}")
        
        # ç»¼åˆæŠ¥å‘Š
        full_results = {
            "method": method,
            "topk_accuracy": topk_results,
            "ranking_quality": ranking_results,
            "score_distribution": score_dist,
        }
        
        # è¯„ä¼°
        print("\nğŸ¯ è¯„ä¼°:")
        top3_acc = topk_results.get("top_3_accuracy", 0)
        mrr = ranking_results.get("mrr", 0)
        
        if top3_acc >= 0.5:
            print("  âœ… ä¼˜ç§€ï¼Top-3å‡†ç¡®ç‡ >= 50%")
        elif top3_acc >= 0.3:
            print("  âš ï¸  ä¸­ç­‰ã€‚Top-3å‡†ç¡®ç‡åœ¨30-50%ä¹‹é—´")
        else:
            print("  âŒ è¾ƒå·®ã€‚Top-3å‡†ç¡®ç‡ < 30%")
        
        if mrr >= 0.5:
            print("  âœ… ä¼˜ç§€ï¼MRR >= 0.5")
        elif mrr >= 0.3:
            print("  âš ï¸  ä¸­ç­‰ã€‚MRRåœ¨0.3-0.5ä¹‹é—´")
        else:
            print("  âŒ è¾ƒå·®ã€‚MRR < 0.3")
        
        return full_results
    
    def compare_methods(self, methods: List[str]) -> Dict[str, Any]:
        """
        æ¯”è¾ƒå¤šä¸ªæ–¹æ³•
        """
        all_results = {}
        
        for method in methods:
            try:
                results = self.evaluate_method(method)
                if results:
                    all_results[method] = results
            except Exception as e:
                print(f"âš ï¸  è¯„ä¼° Method {method} æ—¶å‡ºé”™: {e}")
        
        # ç”Ÿæˆå¯¹æ¯”è¡¨
        if len(all_results) > 1:
            print(f"\n{'='*60}")
            print("ğŸ“Š æ–¹æ³•å¯¹æ¯”")
            print(f"{'='*60}\n")
            
            # Top-Kå‡†ç¡®ç‡å¯¹æ¯”
            print("Top-Kå‡†ç¡®ç‡å¯¹æ¯”:")
            print(f"{'Method':<10} {'Top-1':<10} {'Top-3':<10} {'Top-5':<10} {'MRR':<10}")
            print("-" * 50)
            
            for method, results in sorted(all_results.items()):
                top1 = results["topk_accuracy"].get("top_1_accuracy", 0)
                top3 = results["topk_accuracy"].get("top_3_accuracy", 0)
                top5 = results["topk_accuracy"].get("top_5_accuracy", 0)
                mrr = results["ranking_quality"].get("mrr", 0)
                
                print(f"{method:<10} {top1:<10.2%} {top3:<10.2%} {top5:<10.2%} {mrr:<10.4f}")
            
            # å¾—åˆ†åˆ†å¸ƒå¯¹æ¯”
            print("\nCohen's då¯¹æ¯”:")
            print(f"{'Method':<10} {'Cohen\\'s d':<15} {'è§£é‡Š'}")
            print("-" * 50)
            
            for method, results in sorted(all_results.items()):
                d = results["score_distribution"].get("cohens_d", 0)
                
                if abs(d) < 0.2:
                    interp = "å¯å¿½ç•¥"
                elif abs(d) < 0.5:
                    interp = "å°"
                elif abs(d) < 0.8:
                    interp = "ä¸­ç­‰"
                else:
                    interp = "å¤§"
                
                print(f"{method:<10} {d:<15.4f} {interp}")
        
        return all_results
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜ç»“æœ"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨æ”¹è¿›çš„æŒ‡æ ‡è¯„ä¼°åŒ¹é…æ–¹æ³•")
    parser.add_argument(
        "--method",
        type=str,
        choices=["1a", "1b", "2a", "2b", "all"],
        default="all",
        help="è¦è¯„ä¼°çš„æ–¹æ³•"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="outputs/improved_evaluation_results.json",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    evaluator = ImprovedEvaluator()
    
    if args.method == "all":
        methods = ["1a", "1b", "2a", "2b"]
        results = evaluator.compare_methods(methods)
    else:
        results = {args.method: evaluator.evaluate_method(args.method)}
    
    if results:
        evaluator.save_results(results, args.output)
    else:
        print("\nâš ï¸  æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())


