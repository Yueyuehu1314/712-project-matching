#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¹ç›®è¿›åº¦é‡åŒ–å™¨
é‡åŒ–æ•°æ®æ”¶é›†ã€æ¡£æ¡ˆç”Ÿæˆã€å›¾æ„å»ºç­‰å„ä¸ªé˜¶æ®µçš„æŒ‡æ ‡
"""

import os
import json
import glob
import re
from typing import Dict, List, Tuple
from collections import Counter, defaultdict
import statistics

class ProjectProgressQuantifier:
    """é¡¹ç›®è¿›åº¦é‡åŒ–å™¨"""
    
    def __init__(self):
        self.stats = {
            'data_collection': {},
            'profile_generation': {},
            'graph_construction': {},
            'enhanced_features': {}
        }
    
    def quantify_data_collection(self) -> Dict:
        """é‡åŒ–æ•°æ®æ”¶é›†é˜¶æ®µ"""
        print("ğŸ“Š é‡åŒ–æ•°æ®æ”¶é›†é˜¶æ®µ...")
        
        # é¡¹ç›®æè¿°å¤„ç†
        project_files = glob.glob("project_md/*.md")
        total_projects = len(project_files)
        
        # æå–å®ä½“ç»Ÿè®¡
        all_entities = {
            'skills': [],
            'technologies': [],
            'requirements': [],
            'majors': [],
            'supervisors': [],
            'other': []
        }
        
        total_words = 0
        
        for project_file in project_files:
            try:
                with open(project_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    total_words += len(content.split())
                    
                    # æå–æŠ€èƒ½
                    skills = self._extract_skills_from_text(content)
                    all_entities['skills'].extend(skills)
                    
                    # æå–æŠ€æœ¯
                    technologies = self._extract_technologies_from_text(content)
                    all_entities['technologies'].extend(technologies)
                    
                    # æå–ä¸“ä¸šè¦æ±‚
                    majors = self._extract_majors_from_text(content)
                    all_entities['majors'].extend(majors)
                    
                    # æå–å¯¼å¸ˆ
                    supervisors = self._extract_supervisors_from_text(content)
                    all_entities['supervisors'].extend(supervisors)
                    
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•å¤„ç† {project_file}: {e}")
        
        # ç»Ÿè®¡å»é‡åçš„å®ä½“
        unique_entities = {
            'skills': len(set(all_entities['skills'])),
            'technologies': len(set(all_entities['technologies'])),
            'majors': len(set(all_entities['majors'])),
            'supervisors': len(set(all_entities['supervisors']))
        }
        
        total_entities = sum(unique_entities.values())
        
        self.stats['data_collection'] = {
            'projects_processed': total_projects,
            'total_entities_extracted': total_entities,
            'entity_breakdown': unique_entities,
            'total_words_processed': total_words,
            'avg_words_per_project': total_words / total_projects if total_projects > 0 else 0
        }
        
        return self.stats['data_collection']
    
    def quantify_profile_generation(self) -> Dict:
        """é‡åŒ–æ¡£æ¡ˆç”Ÿæˆé˜¶æ®µ"""
        print("ğŸ‘¥ é‡åŒ–æ¡£æ¡ˆç”Ÿæˆé˜¶æ®µ...")
        
        profile_files = []
        for root, dirs, files in os.walk("profile_md"):
            for file in files:
                if file.endswith('.md'):
                    profile_files.append(os.path.join(root, file))
        
        total_profiles = len(profile_files)
        profile_lengths = []
        total_courses = []
        total_skills = []
        total_projects = []
        
        for profile_file in profile_files[:50]:  # é‡‡æ ·50ä¸ªæ¡£æ¡ˆé¿å…å¤„ç†å¤ªä¹…
            try:
                with open(profile_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # ç»Ÿè®¡å­—æ•°
                    word_count = len(content.split())
                    profile_lengths.append(word_count)
                    
                    # ç»Ÿè®¡è¯¾ç¨‹æ•°é‡
                    courses = self._extract_courses_from_profile(content)
                    total_courses.append(len(courses))
                    
                    # ç»Ÿè®¡æŠ€èƒ½æ•°é‡
                    skills = self._extract_skills_from_profile(content)
                    total_skills.append(len(skills))
                    
                    # ç»Ÿè®¡é¡¹ç›®ç»éªŒæ•°é‡
                    projects = self._extract_project_experience_from_profile(content)
                    total_projects.append(len(projects))
                    
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•å¤„ç† {profile_file}: {e}")
        
        self.stats['profile_generation'] = {
            'total_profiles_created': total_profiles,
            'sampled_profiles_analyzed': len(profile_lengths),
            'avg_profile_length_words': statistics.mean(profile_lengths) if profile_lengths else 0,
            'median_profile_length_words': statistics.median(profile_lengths) if profile_lengths else 0,
            'avg_courses_per_student': statistics.mean(total_courses) if total_courses else 0,
            'avg_skills_per_student': statistics.mean(total_skills) if total_skills else 0,
            'avg_projects_per_student': statistics.mean(total_projects) if total_projects else 0
        }
        
        return self.stats['profile_generation']
    
    def quantify_graph_construction(self) -> Dict:
        """é‡åŒ–å›¾æ„å»ºé˜¶æ®µ"""
        print("ğŸ•¸ï¸ é‡åŒ–å›¾æ„å»ºé˜¶æ®µ...")
        
        # é¡¹ç›®çŸ¥è¯†å›¾è°±ç»Ÿè®¡
        project_kg_stats = self._analyze_kg_directory("individual_kg/projects")
        
        # å­¦ç”ŸçŸ¥è¯†å›¾è°±ç»Ÿè®¡
        student_kg_stats = self._analyze_kg_directory("individual_kg/students")
        
        self.stats['graph_construction'] = {
            'project_kgs': project_kg_stats,
            'student_kgs': student_kg_stats
        }
        
        return self.stats['graph_construction']
    
    def _analyze_kg_directory(self, kg_dir: str) -> Dict:
        """åˆ†æçŸ¥è¯†å›¾è°±ç›®å½•"""
        if not os.path.exists(kg_dir):
            return {'total_graphs': 0, 'avg_nodes': 0, 'avg_edges': 0}
        
        stats_files = glob.glob(os.path.join(kg_dir, "*_stats.json"))
        
        total_graphs = len(stats_files)
        node_counts = []
        edge_counts = []
        entity_type_counts = defaultdict(list)
        relation_type_counts = defaultdict(list)
        
        for stats_file in stats_files:
            try:
                with open(stats_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    node_counts.append(data.get('total_entities', 0))
                    edge_counts.append(data.get('total_relationships', 0))
                    
                    # ç»Ÿè®¡å®ä½“ç±»å‹
                    entity_types = data.get('entity_types', {})
                    for entity_type, count in entity_types.items():
                        entity_type_counts[entity_type].append(count)
                    
                    # ç»Ÿè®¡å…³ç³»ç±»å‹
                    relation_types = data.get('relation_types', {})
                    for relation_type, count in relation_types.items():
                        relation_type_counts[relation_type].append(count)
                        
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•å¤„ç† {stats_file}: {e}")
        
        # è®¡ç®—å¹³å‡å€¼
        avg_entity_types = {}
        for entity_type, counts in entity_type_counts.items():
            avg_entity_types[entity_type] = statistics.mean(counts) if counts else 0
        
        avg_relation_types = {}
        for relation_type, counts in relation_type_counts.items():
            avg_relation_types[relation_type] = statistics.mean(counts) if counts else 0
        
        return {
            'total_graphs': total_graphs,
            'avg_nodes': statistics.mean(node_counts) if node_counts else 0,
            'avg_edges': statistics.mean(edge_counts) if edge_counts else 0,
            'median_nodes': statistics.median(node_counts) if node_counts else 0,
            'median_edges': statistics.median(edge_counts) if edge_counts else 0,
            'avg_entity_types': avg_entity_types,
            'avg_relation_types': avg_relation_types
        }
    
    def quantify_enhanced_features(self) -> Dict:
        """é‡åŒ–å¢å¼ºåŠŸèƒ½"""
        print("âœ¨ é‡åŒ–å¢å¼ºåŠŸèƒ½...")
        
        # æ£€æŸ¥è¯¾ç¨‹-æŠ€èƒ½è¿æ¥
        teaches_skill_count = 0
        total_kg_files = 0
        
        for kg_dir in ["individual_kg/students", "individual_kg/projects"]:
            if os.path.exists(kg_dir):
                relationship_files = glob.glob(os.path.join(kg_dir, "*_relationships.json"))
                total_kg_files += len(relationship_files)
                
                for rel_file in relationship_files:
                    try:
                        with open(rel_file, 'r', encoding='utf-8') as f:
                            relationships = json.load(f)
                            for rel in relationships:
                                if rel.get('relation_type') == 'TEACHES_SKILL':
                                    teaches_skill_count += 1
                    except Exception as e:
                        continue
        
        # æ£€æŸ¥å¯è§†åŒ–æ–‡ä»¶
        visualization_count = len(glob.glob("individual_kg/*/*.png"))
        
        self.stats['enhanced_features'] = {
            'course_skill_connections': teaches_skill_count,
            'total_kg_files_checked': total_kg_files,
            'visualization_files_generated': visualization_count,
            'enhanced_kg_enabled': teaches_skill_count > 0
        }
        
        return self.stats['enhanced_features']
    
    def _extract_skills_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æŠ€èƒ½"""
        skills = []
        text_lower = text.lower()
        
        skill_patterns = [
            'machine learning', 'web development', 'data science', 'cybersecurity',
            'mobile development', 'database', 'networking', 'programming',
            'artificial intelligence', 'blockchain', 'cloud computing'
        ]
        
        for skill in skill_patterns:
            if skill in text_lower:
                skills.append(skill)
        
        return skills
    
    def _extract_technologies_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–æŠ€æœ¯"""
        technologies = []
        text_lower = text.lower()
        
        tech_patterns = [
            'python', 'java', 'javascript', 'react', 'tensorflow', 'sql',
            'mongodb', 'mysql', 'docker', 'kubernetes', 'aws'
        ]
        
        for tech in tech_patterns:
            if tech in text_lower:
                technologies.append(tech)
        
        return technologies
    
    def _extract_majors_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–ä¸“ä¸š"""
        majors = []
        
        major_patterns = [
            r'Computer Science', r'Data Science', r'Software Development',
            r'Business Analysis', r'Cyber Security', r'Enterprise Systems'
        ]
        
        for pattern in major_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                majors.append(pattern)
        
        return majors
    
    def _extract_supervisors_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å¯¼å¸ˆ"""
        supervisors = []
        
        # æŸ¥æ‰¾å¯¼å¸ˆæ¨¡å¼
        supervisor_pattern = r'Academic Supervisor[^|]*\|[^|]*\|([^|]+)'
        matches = re.findall(supervisor_pattern, text, re.IGNORECASE)
        
        for match in matches:
            supervisor = match.strip()
            if supervisor and supervisor != '':
                supervisors.append(supervisor)
        
        return supervisors
    
    def _extract_courses_from_profile(self, content: str) -> List[str]:
        """ä»å­¦ç”Ÿæ¡£æ¡ˆä¸­æå–è¯¾ç¨‹"""
        courses = []
        lines = content.split('\n')
        
        in_courses_section = False
        for line in lines:
            if '## Completed Courses' in line:
                in_courses_section = True
                continue
            elif line.startswith('##'):
                in_courses_section = False
            elif in_courses_section and line.strip().startswith('-'):
                course = line.strip().lstrip('- ').strip()
                if course:
                    courses.append(course)
        
        return courses
    
    def _extract_skills_from_profile(self, content: str) -> List[str]:
        """ä»å­¦ç”Ÿæ¡£æ¡ˆä¸­æå–æŠ€èƒ½"""
        skills = []
        lines = content.split('\n')
        
        in_skills_section = False
        for line in lines:
            if '## Technical Skills' in line:
                in_skills_section = True
                continue
            elif line.startswith('##'):
                in_skills_section = False
            elif in_skills_section and line.strip().startswith('-'):
                skill = line.strip().lstrip('- ').strip()
                if skill:
                    skills.append(skill)
        
        return skills
    
    def _extract_project_experience_from_profile(self, content: str) -> List[str]:
        """ä»å­¦ç”Ÿæ¡£æ¡ˆä¸­æå–é¡¹ç›®ç»éªŒ"""
        projects = []
        lines = content.split('\n')
        
        in_projects_section = False
        for line in lines:
            if '## Project Experience' in line:
                in_projects_section = True
                continue
            elif line.startswith('##'):
                in_projects_section = False
            elif in_projects_section and line.strip().startswith('###'):
                project = line.strip().lstrip('### ').strip()
                if project:
                    projects.append(project)
        
        return projects
    
    def generate_progress_report(self) -> str:
        """ç”Ÿæˆè¿›åº¦æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆç»¼åˆè¿›åº¦æŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®
        data_collection = self.quantify_data_collection()
        profile_generation = self.quantify_profile_generation()
        graph_construction = self.quantify_graph_construction()
        enhanced_features = self.quantify_enhanced_features()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# é¡¹ç›®åŒ¹é…ç³»ç»Ÿ - è¿›åº¦é‡åŒ–æŠ¥å‘Š

## ğŸ“Š æ•°æ®æ”¶é›†é˜¶æ®µ (Data Collection)
- **é¡¹ç›®æè¿°å¤„ç†**: {data_collection['projects_processed']} project descriptions processed
- **å®ä½“æå–æ€»æ•°**: {data_collection['total_entities_extracted']} entities extracted
  - Skills: {data_collection['entity_breakdown']['skills']}
  - Technologies: {data_collection['entity_breakdown']['technologies']} 
  - Majors: {data_collection['entity_breakdown']['majors']}
  - Supervisors: {data_collection['entity_breakdown']['supervisors']}
- **æ–‡æœ¬å¤„ç†**: {data_collection['total_words_processed']:,} words processed
- **å¹³å‡é¡¹ç›®é•¿åº¦**: {data_collection['avg_words_per_project']:.0f} words per project

## ğŸ‘¥ æ¡£æ¡ˆç”Ÿæˆé˜¶æ®µ (Profile Generation)
- **å­¦ç”Ÿæ¡£æ¡ˆåˆ›å»º**: {profile_generation['total_profiles_created']} synthetic student profiles created
- **å¹³å‡æ¡£æ¡ˆé•¿åº¦**: {profile_generation['avg_profile_length_words']:.0f} words per profile
- **æ¡£æ¡ˆè´¨é‡æŒ‡æ ‡**:
  - Average courses per student: {profile_generation['avg_courses_per_student']:.1f}
  - Average skills per student: {profile_generation['avg_skills_per_student']:.1f}
  - Average projects per student: {profile_generation['avg_projects_per_student']:.1f}

## ğŸ•¸ï¸ å›¾æ„å»ºé˜¶æ®µ (Graph Construction)

### é¡¹ç›®çŸ¥è¯†å›¾è°± (Project KGs)
- **é¡¹ç›®å›¾è°±æ•°é‡**: {graph_construction['project_kgs']['total_graphs']} project KGs constructed
- **å¹³å‡èŠ‚ç‚¹æ•°**: {graph_construction['project_kgs']['avg_nodes']:.1f} nodes per project
- **å¹³å‡è¾¹æ•°**: {graph_construction['project_kgs']['avg_edges']:.1f} edges per project

### å­¦ç”ŸçŸ¥è¯†å›¾è°± (Student KGs)  
- **å­¦ç”Ÿå›¾è°±æ•°é‡**: {graph_construction['student_kgs']['total_graphs']} student KGs built
- **å¹³å‡èŠ‚ç‚¹æ•°**: {graph_construction['student_kgs']['avg_nodes']:.1f} nodes per student
- **å¹³å‡è¾¹æ•°**: {graph_construction['student_kgs']['avg_edges']:.1f} edges per student
"""

        # æ·»åŠ å®ä½“ç±»å‹åˆ†å¸ƒ
        if graph_construction['student_kgs']['avg_entity_types']:
            report += "\n### å­¦ç”Ÿå›¾è°±å®ä½“åˆ†å¸ƒ:\n"
            for entity_type, avg_count in graph_construction['student_kgs']['avg_entity_types'].items():
                if avg_count > 0:
                    report += f"  - {entity_type}: {avg_count:.1f} average per student\n"

        # æ·»åŠ å¢å¼ºåŠŸèƒ½ç»Ÿè®¡
        report += f"""
## âœ¨ å¢å¼ºåŠŸèƒ½é˜¶æ®µ (Enhanced Features)
- **è¯¾ç¨‹-æŠ€èƒ½è¿æ¥**: {enhanced_features['course_skill_connections']} TEACHES_SKILL relationships created
- **å¯è§†åŒ–æ–‡ä»¶**: {enhanced_features['visualization_files_generated']} visualization files generated
- **å¢å¼ºKGçŠ¶æ€**: {'âœ… Enabled' if enhanced_features['enhanced_kg_enabled'] else 'âŒ Not enabled'}

## ğŸ“ˆ æ€»ä½“è¿›å±•æ‘˜è¦
- âœ… **æ•°æ®æ”¶é›†**: {data_collection['projects_processed']} projects, {data_collection['total_entities_extracted']} entities
- âœ… **æ¡£æ¡ˆç”Ÿæˆ**: {profile_generation['total_profiles_created']} profiles, avg {profile_generation['avg_profile_length_words']:.0f} words
- âœ… **å›¾æ„å»º**: {graph_construction['project_kgs']['total_graphs']} project KGs + {graph_construction['student_kgs']['total_graphs']} student KGs
- {'âœ…' if enhanced_features['enhanced_kg_enabled'] else 'ğŸ”„'} **å¢å¼ºåŠŸèƒ½**: Courseâ†’Skill connections {'implemented' if enhanced_features['enhanced_kg_enabled'] else 'in progress'}

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ é¡¹ç›®è¿›åº¦é‡åŒ–å™¨")
    print("=" * 60)
    
    quantifier = ProjectProgressQuantifier()
    report = quantifier.generate_progress_report()
    
    # ä¿å­˜æŠ¥å‘Š
    with open("progress_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(report)
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: progress_report.md")

if __name__ == "__main__":
    main()








